{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42438f17",
   "metadata": {},
   "source": [
    "# Integrating Big Data Analytics and Convolutional Neural Networks for Pest and Disease Detection and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ead59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the necessary libraries\n",
    "#!pip install torch\n",
    "#!pip install torchvision\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d941a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ef018b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/26 12:59:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7266cc299cc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a spark session\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Load Image Data\").getOrCreate()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae524128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load image data\n",
    "image_df = spark.read.format(\"image\").load(\"hdfs://localhost:9000/crop_pest_disease_dataset/Test\")\n",
    "\n",
    "# Collect 2 images as Pandas DataFrame\n",
    "images_pd = image_df.limit(2).toPandas()\n",
    "\n",
    "# Display the images\n",
    "for index, row in images_pd.iterrows():\n",
    "    image = cv2.imread(row['image.origin'])\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46357baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = spark.read.format(\"image\").load(\"hdfs://localhost:9000/crop_pest_disease_dataset/Test\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab13c37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n",
      "+-----+\n",
      "|image|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df.printSchema()\n",
    "image_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f069643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "IMAGE_SIZE = (128, 128)\n",
    "INPUT_SHAPE = (128, 128, 3)\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_IMAGES_PER_CATEGORY = 5000\n",
    "CLASS_NAMES = ['Cashew', 'Cassava', 'Maize', 'Tomato']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36681ca",
   "metadata": {},
   "source": [
    "### Get the plant and disease category from the file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a305b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def get_disease_category(file_path):\n",
    "    return file_path.split(\"/\")[5]\n",
    "\n",
    "def get_plant_category(file_path):\n",
    "    disease_category = get_disease_category(file_path)\n",
    "    return disease_category.split(\" \")[0]\n",
    "\n",
    "# Define UDFs\n",
    "get_disease_category_udf = udf(get_disease_category, StringType())\n",
    "get_plant_category_udf = udf(get_plant_category, StringType())\n",
    "\n",
    "# Apply UDFs to DataFrame\n",
    "dataset_path_with_categories = image_df.withColumn(\"disease_category\", get_disease_category_udf(image_df[\"image.origin\"])) \\\n",
    "                                           .withColumn(\"plant_category\", get_plant_category_udf(image_df[\"image.origin\"]))\n",
    "\n",
    "# Show the schema and display first few rows with new categories\n",
    "dataset_path_with_categories.printSchema()\n",
    "dataset_path_with_categories.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5fe887",
   "metadata": {},
   "source": [
    "## Convert the dataset of images to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4357c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2494bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Define the image processing function\n",
    "def process_images(images):\n",
    "    IMAGE_SIZE = (224, 224)  # Example image size\n",
    "\n",
    "    def process_single_image(filename):\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.io.decode_jpeg(img)\n",
    "        img = tf.image.resize(img, IMAGE_SIZE)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        return img.numpy().tolist()\n",
    "\n",
    "    return [process_single_image(filename) for filename in images]\n",
    "\n",
    "# Define UDF for image processing\n",
    "process_images_udf = udf(process_images, ArrayType(StringType()))\n",
    "\n",
    "# Apply UDF to DataFrame\n",
    "processed_images_df = dataset_path_with_categories.withColumn(\"processed_images\", process_images_udf(dataset_path_with_categories[\"image.origin\"]))\n",
    "\n",
    "# Show the schema and display first few rows with processed images\n",
    "processed_images_df.printSchema()\n",
    "processed_images_df.show(1, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "# Define the image processing function\n",
    "def process_images(images):\n",
    "    IMAGE_SIZE = (224, 224)  # Example image size\n",
    "\n",
    "    def process_single_image(filename):\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.io.decode_jpeg(img)\n",
    "        img = tf.image.resize(img, IMAGE_SIZE)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        return img.numpy().tolist()\n",
    "\n",
    "    return [process_single_image(filename) for filename in images]\n",
    "\n",
    "# Define UDF for image processing\n",
    "process_images_udf = udf(process_images, ArrayType(StringType()))\n",
    "\n",
    "# Apply UDF to DataFrame\n",
    "processed_images_df = dataset_path_with_categories.withColumn(\"processed_images\", process_images_udf(dataset_path_with_categories[\"image.origin\"]))\n",
    "\n",
    "# Define a UDF to check if an image is valid\n",
    "def is_valid_image(img):\n",
    "    try:\n",
    "        # Attempt to create a TensorFlow image from the data\n",
    "        tf.io.decode_jpeg(tf.convert_to_tensor(img))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Define UDF for image validity\n",
    "is_valid_image_udf = udf(is_valid_image)\n",
    "\n",
    "# Add a new column indicating whether each image is valid or not\n",
    "processed_images_df = processed_images_df.withColumn(\"is_valid_image\", is_valid_image_udf(processed_images_df[\"processed_images\"]))\n",
    "\n",
    "# Show the schema and display first few rows with processed images and their validity\n",
    "processed_images_df.printSchema()\n",
    "processed_images_df.show(1, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, BooleanType\n",
    "\n",
    "# Define the image processing function\n",
    "def process_images(images):\n",
    "    IMAGE_SIZE = (224, 224)  # Example image size\n",
    "\n",
    "    def process_single_image(filename):\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.io.decode_jpeg(img)\n",
    "        img = tf.image.resize(img, IMAGE_SIZE)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        return img.numpy().tolist()\n",
    "\n",
    "    return [process_single_image(filename) for filename in images]\n",
    "\n",
    "# Define UDF for image processing\n",
    "process_images_udf = udf(process_images, ArrayType(StringType()))\n",
    "\n",
    "# Apply UDF to DataFrame\n",
    "processed_images_df = dataset_path_with_categories.withColumn(\"processed_images\", process_images_udf(dataset_path_with_categories[\"image.origin\"]))\n",
    "\n",
    "# Define a UDF to check if an image is valid\n",
    "def is_valid_image(img):\n",
    "    try:\n",
    "        # Attempt to create a TensorFlow image from the data\n",
    "        tf.io.decode_jpeg(tf.convert_to_tensor(img))\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Define UDF for image validity\n",
    "is_valid_image_udf = udf(is_valid_image, BooleanType())\n",
    "\n",
    "# Add a new column indicating whether each image is valid or not\n",
    "processed_images_df = processed_images_df.withColumn(\"is_valid_image\", is_valid_image_udf(processed_images_df[\"processed_images\"]))\n",
    "\n",
    "# Drop rows where the image is not valid\n",
    "processed_images_df = processed_images_df.filter(processed_images_df[\"is_valid_image\"])\n",
    "\n",
    "# Show the schema and display first few rows with processed images and their validity\n",
    "processed_images_df.printSchema()\n",
    "processed_images_df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e991b00b",
   "metadata": {},
   "source": [
    "## Encode the target label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Define the label encoding function\n",
    "def encode_label(label):\n",
    "    if label == 'Cashew':\n",
    "        return 0\n",
    "    elif label == 'Cassava':\n",
    "        return 1\n",
    "    elif label == 'Maize':\n",
    "        return 2\n",
    "    elif label == 'Tomato':\n",
    "        return 3\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Define UDF for label encoding\n",
    "encode_label_udf = udf(encode_label, IntegerType())\n",
    "\n",
    "# Apply UDF to DataFrame to create a new column for encoded labels\n",
    "processed_images_df = processed_images_df.withColumn(\"encoded_label\", encode_label_udf(processed_images_df[\"plant_category\"]))\n",
    "\n",
    "# Show the schema and display first few rows with encoded labels\n",
    "processed_images_df.printSchema()\n",
    "processed_images_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ed91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect filenames, plant categories, and disease categories into lists\n",
    "filenames = processed_images_df.select(\"image.origin\").rdd.flatMap(lambda x: x).collect()\n",
    "plant_category = processed_images_df.select(\"plant_category\").rdd.flatMap(lambda x: x).collect()\n",
    "disease_category = processed_images_df.select(\"disease_category\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Show the first few elements of each list for verification\n",
    "print(\"Filenames:\", filenames[:5])\n",
    "print(\"Plant categories:\", plant_category[:5])\n",
    "print(\"Disease categories:\", disease_category[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a Pandas dataframe using the lists\n",
    "data = {\"filename\": filenames, \"plant_category\": plant_category, \"disease_category\": disease_category}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Print the first 10 rows of the shuffled dataframe\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ecdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_sizes = df.groupby(\"plant_category\").size()\n",
    "print(category_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4356be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "categories = df['plant_category'].sort_values().unique()\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# creating the bar plot\n",
    "plt.bar(categories, category_sizes, color ='blue',\n",
    "        width = 0.4)\n",
    "\n",
    "plt.xlabel(\"Plant Categories\")\n",
    "plt.ylabel(\"No. of Images\")\n",
    "plt.title(\"Number of Images Per Plant Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca872612",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd056dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the directory containing the image files on your local machine\n",
    "local_image_path = \"file:///home/hduser/crop_pest_disease_dataset/Train\"\n",
    "\n",
    "# Read binary files into DataFrame\n",
    "image_df = spark.read.binaryFiles(local_image_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "image_df.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024fd8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "/crop_pest_disease_data/Cassava/test_set\n",
    "/crop_pest_disease_data/Cassava/train_set\n",
    "Files in /crop_pest_disease_data/Cashew directory:\n",
    "/crop_pest_disease_data/Cashew/test_set\n",
    "/crop_pest_disease_data/Cashew/train_set\n",
    "Files in /crop_pest_disease_data/Maize directory:\n",
    "/crop_pest_disease_data/Maize/test_set\n",
    "/crop_pest_disease_data/Maize/train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassava_train = \"/crop_pest_disease_data/Cassava/train_set\"\n",
    "cassavatrain_df = spark.read.format(\"image\").load(cassava_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassavatrain_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c1f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Read images from HDFS\n",
    "image_rdd = sc.binaryFiles(\"/crop_pest_disease_data/Cassava/train_set\")\n",
    "\n",
    "# Process images\n",
    "def process_image(image_data):\n",
    "    img = Image.open(io.BytesIO(image_data))\n",
    "    # Perform image processing tasks (e.g., resize, filter)\n",
    "    # Example: resized_img = img.resize((new_width, new_height))\n",
    "    return img\n",
    "\n",
    "processed_images_rdd = image_rdd.map(lambda x: process_image(x[1]))\n",
    "\n",
    "# Save processed images back to HDFS\n",
    "processed_images_rdd.saveAsSequenceFile(\"hdfs://path/to/processed_images\")\n",
    "\n",
    "# Stop SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fe29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(cassavatrain_df.select(\"image.*\").schema.fields + [\n",
    "    StructField(\"data_as_resized_array\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"data_as_array\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "def resize_img(img_data, resize=True):\n",
    "    mode = 'RGBA' if (img_data.nChannels == 4) else 'RGB' \n",
    "    img = Image.frombytes(mode=mode, data=img_data.data, size=[img_data.width, img_data.height])\n",
    "    img = img.convert('RGB') if (mode == 'RGBA') else img\n",
    "    img = img.resize([224, 224], resample=Image.Resampling.BICUBIC) if (resize) else img\n",
    "    arr = convert_bgr_array_to_rgb_array(np.asarray(img))\n",
    "    arr = arr.reshape([224*224*3]) if (resize) else arr.reshape([img_data.width*img_data.height*3])\n",
    "\n",
    "    return arr\n",
    "\n",
    "def resize_image_udf(dataframe_batch_iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    for dataframe_batch in dataframe_batch_iterator:\n",
    "        dataframe_batch[\"data_as_resized_array\"] = dataframe_batch.apply(resize_img, args=(True,), axis=1)\n",
    "        dataframe_batch[\"data_as_array\"] = dataframe_batch.apply(resize_img, args=(False,), axis=1)\n",
    "        yield dataframe_batch\n",
    "\n",
    "resized_df = image_df.select(\"image.*\").mapInPandas(resize_image_udf, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001dc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the collected list\n",
    "collected_images = cassavatrain_df.select(\"image\").collect()\n",
    "num_images = len(collected_images)\n",
    "\n",
    "# Adjust the index if it's out of range\n",
    "if image_row >= num_images:\n",
    "    print(f\"Index {image_row} is out of range. Adjusting to {num_images - 1}.\")\n",
    "    image_row = num_images - 1\n",
    "\n",
    "# Access the image at the adjusted index\n",
    "spark_single_img = collected_images[image_row]\n",
    "(spark_single_img.image.origin, spark_single_img.image.mode, spark_single_img.image.nChannels )\n",
    "mode = 'RGBA' if (spark_single_img.image.nChannels == 4) else 'RGB'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091cf35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the collected list\n",
    "collected_images = cassavatrain_df.select(\"image\").collect()\n",
    "if image_row < len(collected_images):\n",
    "    spark_single_img = collected_images[image_row]\n",
    "    (spark_single_img.image.origin, spark_single_img.image.mode, spark_single_img.image.nChannels )\n",
    "    \n",
    "    mode = 'RGBA' if (spark_single_img.image.nChannels == 4) else 'RGB'\n",
    "else:\n",
    "    print(\"Error: Index out of range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c2e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the collected list\n",
    "collected_images = cassavatrain_df.select(\"image\").collect()\n",
    "num_images = len(collected_images)\n",
    "\n",
    "# Adjust the index if it's out of range\n",
    "if image_row >= num_images:\n",
    "    print(f\"Index {image_row} is out of range. Adjusting to {num_images - 1}.\")\n",
    "    image_row = num_images - 1\n",
    "\n",
    "# Access the image at the adjusted index\n",
    "spark_single_img = collected_images[image_row]\n",
    "(spark_single_img.image.origin, spark_single_img.image.mode, spark_single_img.image.nChannels )\n",
    "mode = 'RGBA' if (spark_single_img.image.nChannels == 4) else 'RGB'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e205b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_row = 40\n",
    "spark_single_img = cassavatrain_df.select(\"image\").collect()[image_row]\n",
    "(spark_single_img.image.origin, spark_single_img.image.mode, spark_single_img.image.nChannels )\n",
    "\n",
    "mode = 'RGBA' if (spark_single_img.image.nChannels == 4) else 'RGB' \n",
    "Image.frombytes(mode=mode, data=bytes(spark_single_img.image.data), size=[spark_single_img.image.width,spark_single_img.image.height]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9312755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_path) and os.listdir(class_path):  # Check if directory exists and not empty\n",
    "            for filename in os.listdir(class_path):\n",
    "                # Process image files here\n",
    "                # Example: load image using OpenCV or any other library\n",
    "                image = cv2.imread(os.path.join(class_path, filename))\n",
    "                # images.append(image)\n",
    "                images.append(os.path.join(class_path, filename))  # Append image file path\n",
    "                labels.append(class_name)\n",
    "        else:\n",
    "            print(f\"Warning: Directory {class_path} is empty or does not exist.\")\n",
    "    return images, labels\n",
    "\n",
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose3102\", \"gumosis1714\", \"healthy5877\", \"leaf miner3466\", \"red rust4751\"],\n",
    "    \"Cassava\": [\"bacterial blight3241\",\"bacterial_blight\", \"brown_spot\", \"green_mite\", \"healthy\", \"mosaic\"],\n",
    "    \"Maize\": [\"fall armyworm\", \"grasshopper\", \"healthy\", \"leaf beetle\", \"leaf blight\", \"leaf_spot\", \"streak virus\"],\n",
    "    \"Tomato\": [\"healthy\", \"leaf blight\", \"leaf curl\", \"septoria leaf spot\", \"verticillium wilt\"]\n",
    "}\n",
    "\n",
    "data_directory = \"hdfs://localhost:9000/crop_pest_disease_data\"\n",
    "\n",
    "train_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_rain_data_rdd = []\n",
    "    for class_name in classes:\n",
    "        hdfs_path = f\"{data_directory}/{crop}/train_set/{class_name}\"\n",
    "        images, labels = load_images_from_directory(hdfs_path)\n",
    "        if images and labels:  # Check if images and labels are not empty\n",
    "            crop_test_data_rdd.append((images, labels))\n",
    "    train_data_rdds.append(crop_train_data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store features and labels\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Iterate over each RDD\n",
    "for crop_rdd_list in train_data_rdds:\n",
    "    for rdd in crop_rdd_list:\n",
    "        # Extract crop name from RDD path\n",
    "        crop = rdd.name().split('/')[-3]\n",
    "        # Extract class name from RDD path\n",
    "        class_name = rdd.name().split('/')[-1]\n",
    "        # Extract features (images) and labels (class names) from file paths\n",
    "        features_with_paths = rdd.collect()\n",
    "        if features_with_paths:  # Check if RDD is not empty\n",
    "            features = [features for _, features in features_with_paths]\n",
    "            labels = [class_name] * len(features)\n",
    "            # Append features and labels to the lists\n",
    "            features_list.extend(features)\n",
    "            labels_list.extend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521ca37",
   "metadata": {},
   "source": [
    "## Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ed91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_path) and os.listdir(class_path):  # Check if directory exists and not empty\n",
    "            for filename in os.listdir(class_path):\n",
    "                # Process image files here\n",
    "                # Example: load image using OpenCV or any other library\n",
    "                # image = cv2.imread(os.path.join(class_path, filename))\n",
    "                # images.append(image)\n",
    "                images.append(os.path.join(class_path, filename))  # Append image file path\n",
    "                labels.append(class_name)\n",
    "        else:\n",
    "            print(f\"Warning: Directory {class_path} is empty or does not exist.\")\n",
    "    return images, labels\n",
    "\n",
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose\", \"gumosis\", \"healthy\", \"leaf miner\", \"red rust\"],\n",
    "    \"Cassava\": [\"bacterial_blight\", \"brown_spot\", \"green_mite\", \"healthy\", \"mosaic\"],\n",
    "    \"Maize\": [\"fall armyworm\", \"grasshopper\", \"healthy\", \"leaf beetle\", \"leaf blight\", \"leaf_spot\", \"streak virus\"],\n",
    "    \"Tomato\": [\"healthy\", \"leaf blight\", \"leaf curl\", \"septoria leaf spot\", \"verticillium wilt\"]\n",
    "}\n",
    "\n",
    "data_directory = \"hdfs://localhost:9000/crop_pest_disease_data\"\n",
    "\n",
    "test_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_test_data_rdd = []\n",
    "    for class_name in classes:\n",
    "        hdfs_path = f\"{data_directory}/{crop}/test_set/{class_name}\"\n",
    "        images, labels = load_images_from_directory(hdfs_path)\n",
    "        if images and labels:  # Check if images and labels are not empty\n",
    "            crop_test_data_rdd.append((images, labels))\n",
    "    test_data_rdds.append(crop_test_data_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a480cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae8fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c4081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a82a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = \"hdfs://localhost:9000/crop_pest_disease_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57249f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os.path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2a5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_path_to_df(hdfs_path):\n",
    "    image_dir = Path(hdfs_path)\n",
    "\n",
    "    # Get filepaths and labels\n",
    "    filepaths = list(image_dir.glob(r'**/*.JPG')) \n",
    "\n",
    "    labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "\n",
    "    filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "    labels = pd.Series(labels, name='Label')\n",
    "\n",
    "    # Concatenate filepaths and labels\n",
    "    image_df = pd.concat([filepaths, labels], axis=1)\n",
    "    return image_df\n",
    "\n",
    "image_df = convert_path_to_df(hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eec8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43291994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for corrupted images within the dataset\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "path = Path(hdfs_path).rglob(\"*.jpg\")\n",
    "for img_p in path:\n",
    "    try:\n",
    "        img = PIL.Image.open(img_p)\n",
    "    except PIL.UnidentifiedImageError:\n",
    "            print(img_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fc8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = image_df['Label'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values, alpha=0.8, palette='rocket')\n",
    "plt.title('Distribution of Labels in Image Dataset', fontsize=16)\n",
    "plt.xlabel('Label', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xticks(rotation=45) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e075dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a7018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ba13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83459a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3933d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c45270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4f55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store features and labels\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Iterate over each RDD\n",
    "for crop_rdd_list in train_data_rdds:\n",
    "    for rdd in crop_rdd_list:\n",
    "        # Extract crop name from RDD path\n",
    "        crop = rdd.name().split('/')[-3]\n",
    "        # Extract class name from RDD path\n",
    "        class_name = rdd.name().split('/')[-1]\n",
    "        # Extract features (images) and labels (class names) from file paths\n",
    "        features_with_paths = rdd.collect()\n",
    "        if features_with_paths:  # Check if RDD is not empty\n",
    "            features = [features for _, features in features_with_paths]\n",
    "            labels = [class_name] * len(features)\n",
    "            # Append features and labels to the lists\n",
    "            features_list.extend(features)\n",
    "            labels_list.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aface619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_path) and os.listdir(class_path):  # Check if directory exists and not empty\n",
    "            for filename in os.listdir(class_path):\n",
    "                # Process image files here\n",
    "                # Example: load image using OpenCV or any other library\n",
    "                # image = cv2.imread(os.path.join(class_path, filename))\n",
    "                # images.append(image)\n",
    "                images.append(os.path.join(class_path, filename))  # Append image file path\n",
    "                labels.append(class_name)\n",
    "        else:\n",
    "            print(f\"Warning: Directory {class_path} is empty or does not exist.\")\n",
    "    return images, labels\n",
    "\n",
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose\", \"gumosis\", \"healthy\", \"leaf miner\", \"red rust\"],\n",
    "    \"Cassava\": [\"bacterial_blight\", \"brown_spot\", \"green_mite\", \"healthy\", \"mosaic\"],\n",
    "    \"Maize\": [\"fall armyworm\", \"grasshopper\", \"healthy\", \"leaf beetle\", \"leaf blight\", \"leaf_spot\", \"streak virus\"],\n",
    "    \"Tomato\": [\"healthy\", \"leaf blight\", \"leaf curl\", \"septoria leaf spot\", \"verticillium wilt\"]\n",
    "}\n",
    "\n",
    "data_directory = \"hdfs://localhost:9000/crop_pest_disease_data\"\n",
    "\n",
    "test_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_test_data_rdd = []\n",
    "    for class_name in classes:\n",
    "        hdfs_path = f\"{data_directory}/{crop}/test_set/{class_name}\"\n",
    "        images, labels = load_images_from_directory(hdfs_path)\n",
    "        if images and labels:  # Check if images and labels are not empty\n",
    "            crop_test_data_rdd.append((images, labels))\n",
    "    test_data_rdds.append(crop_test_data_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c490f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb626ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hadoop directory path containing image files\n",
    "hadoop_dir_path = \"hdfs://localhost:9000/crop_pest_disease_data\"\n",
    "\n",
    "# Read image files from Hadoop directory\n",
    "image_df = spark.read.format(\"image\").load(hadoop_dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc143565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a snippet of the DataFrame\n",
    "image_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d45743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View DataFrame schema\n",
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa70b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_df = datasets.ImageFolder(root=hadoop_data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e59238",
   "metadata": {},
   "source": [
    "## Perform Image processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2143f",
   "metadata": {},
   "source": [
    "The dataset has already been processed. Here is the image preprocessing that was done to the data.\n",
    "- All images were captured, separated,and saved in their respective folders according to the plant type.\n",
    "- The images were annotated and labelled\n",
    "- Image Cropping and size reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cba35",
   "metadata": {},
   "source": [
    "## Data Processing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d1c54",
   "metadata": {},
   "source": [
    "### Checking the image sizes of each crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have already defined and loaded your dataset\n",
    "# dataset = datasets.ImageFolder(root=hadoop_data_path, transform=transform)\n",
    "\n",
    "# Get the list of classes (crops) in the dataset\n",
    "classes = dataset.classes\n",
    "\n",
    "# Create a dictionary to store image sizes for each crop\n",
    "crop_image_sizes = {crop: [] for crop in classes}\n",
    "\n",
    "# Iterate through the dataset\n",
    "for image_path, label in dataset.samples:\n",
    "    # Open the image using PIL\n",
    "    image = Image.open(image_path)\n",
    "    # Get the size of the image\n",
    "    width, height = image.size\n",
    "    # Get the crop name using the label\n",
    "    crop_name = classes[label]\n",
    "    # Append the image size to the corresponding crop in the dictionary\n",
    "    crop_image_sizes[crop_name].append((width, height))\n",
    "\n",
    "# Print the image sizes for each crop\n",
    "for crop, sizes in crop_image_sizes.items():\n",
    "    print(f\"Crop: {crop}\")\n",
    "    print(f\"Total images: {len(sizes)}\")\n",
    "    print(f\"Average image size: {sum([w for w, h in sizes]) / len(sizes)} x {sum([h for w, h in sizes]) / len(sizes)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e23b5",
   "metadata": {},
   "source": [
    "### check if the pixel value have been normalized to a range suitable for training neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641935e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Assuming you have already defined and loaded your dataset\n",
    "# dataset = datasets.ImageFolder(root=hadoop_data_path, transform=transform)\n",
    "\n",
    "# Define a transformation to convert PIL images to PyTorch tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load a few sample images from the dataset\n",
    "sample_loader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "for images, labels in sample_loader:\n",
    "    # Convert images to numpy arrays and print pixel value range\n",
    "    for image in images:\n",
    "        # Check pixel value range\n",
    "        min_pixel_value = torch.min(image)\n",
    "        max_pixel_value = torch.max(image)\n",
    "        print(f\"Min pixel value: {min_pixel_value}, Max pixel value: {max_pixel_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bd5d9",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define transformations for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load image data using PyTorch\n",
    "hadoop_data_path = \"hdfs://path/to/crop_pest_disease_data\"\n",
    "dataset = datasets.ImageFolder(root=hadoop_data_path, transform=transform)\n",
    "\n",
    "# Split data into training and testing datasets\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b16332",
   "metadata": {},
   "source": [
    "## Check if there are any defective images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9948d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='hdfs://localhost:9000/crop_pest_disease_data'\n",
    "bad_img_list=[]\n",
    "total=0\n",
    "good=0\n",
    "bad=0\n",
    "classes=sorted(os.listdir(data_dir))\n",
    "for klass in classes:\n",
    "    good_class=0\n",
    "    bad_class=0\n",
    "    total_class=0\n",
    "    msg=f'processing class {klass}'\n",
    "    print(msg, '\\r', end= '')\n",
    "    classpath=os.path.join(data_dir, klass)\n",
    "    flist=sorted(os.listdir(classpath))\n",
    "    for f in flist:\n",
    "        total +=1\n",
    "        total_class +=1\n",
    "        fpath=os.path.join(classpath,f)\n",
    "        try:\n",
    "            img= Image.open(fpath) \n",
    "            array=np.asarray(img)\n",
    "            good +=1\n",
    "            good_class +=1\n",
    "        except:\n",
    "            bad_img_list.append(fpath)\n",
    "            bad +=1\n",
    "            bad_class +=1\n",
    "    \n",
    "    msg=f'class {klass} contains {total_class} files, {good_class} are valid image files and {bad_class} defective image files'\n",
    "    print (msg)\n",
    "msg=f'the dataset contains {total} image files, {good} are valid image files and {bad} are defective image files'\n",
    "print (msg)\n",
    "if bad>0:\n",
    "    ans=input('to print a list of defective image files enter P, to not print press Enter')\n",
    "    if ans == 'P' or ans == 'p':\n",
    "        for f in bad_img_list:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d62de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create a corrected dataset with the defective image files removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use this dataset to create a model.\n",
    "working_dir=r'/kaggle/working/'\n",
    "corrected_dir=os.path.join(working_dir, 'corrected dataset') # where the corrected dataset will be stored\n",
    "copied_count = 0\n",
    "if os.path.isdir(corrected_dir):\n",
    "    shutil.rmtree(corrected_dir) # make sure the corrected_dir is empty\n",
    "os.mkdir(corrected_dir)\n",
    "for klass in classes:\n",
    "    classpath=os.path.join(data_dir, klass)\n",
    "    dest_classpath=os.path.join(corrected_dir, klass)\n",
    "    os.mkdir(dest_classpath)\n",
    "    flist= os.listdir(classpath)\n",
    "    for f in flist:\n",
    "        fpath=os.path.join(classpath,f)\n",
    "        dest_fpath=os.path.join(dest_classpath,f)\n",
    "        if fpath not in bad_img_list:\n",
    "            shutil.copy(fpath, dest_fpath)\n",
    "            copied_count +=1\n",
    "msg=f'{copied_count} valid image files were stored in {corrected_dir}'\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the training data into training and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa030c",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc354a6",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b2c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32b442f2",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ae5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose\", \"gumosis\", \"healthy\", \"leaf miner\", \"red rust\"],\n",
    "    \"Cassava\": [\"bacterial_blight\", \"brown_spot\", \"green_mite\", \"healthy\",\"mosaic\"],\n",
    "    \"Maize\": [\"fall armyworm\", \"grasshoper\", \"healthy\", \"leaf beetle\", \"leaf blight\", \"leaf_spot\",\"streak virus\"],\n",
    "    \"Tomato\": [\"healthy\", \"leaf blight\", \"leaf curl\", \"septoria leaf spot\",\"verticulium wilt\"]\n",
    "}\n",
    "\n",
    "test_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_test_data_rdd = []\n",
    "    for class_name in classes:\n",
    "\n",
    "\n",
    "        hdfs_path = f\"hdfs://localhost:9000/crop_pest_disease_data/{crop}/test_set/{class_name}\"\n",
    "        test_images_rdd = spark.sparkContext.binaryFiles(hdfs_path)\n",
    "        crop_test_data_rdd.append(test_images_rdd)\n",
    "    test_data_rdds.append(crop_test_data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db97b5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b786c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = f\"hdfs://localhost:9000/crop_pest_disease_data/Maize/test_set/grasshopper/0maize_valid_grasshoper.JPG\"\n",
    "test_images = spark.sparkContext.binaryFiles(hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=test_images.collect()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c816b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is from the previous example.\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Path to your image data\n",
    "cropdata = \"file:///home/hduser/Big data and advanced analytics data\"\n",
    "\n",
    "# Read image files into a DataFrame\n",
    "crop_dataframe = spark.read.format(\"image\").load(cropdata)\n",
    "\n",
    "# Show the DataFrame schema and first few rows\n",
    "crop_dataframe.printSchema()\n",
    "crop_dataframe.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480df35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd6676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571a636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89eae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"comment\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbfd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all libraries required for EDA, Preprocessing, Model building, Model Testing, Model EValuation and Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default size for input images\n",
    "width=256\n",
    "height=256\n",
    "depth=3epoch_ = 25\n",
    "BS = 32\n",
    "default_image_size = tuple((256, 256))\n",
    "image_size = 0\n",
    "root_dir = '/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/'\n",
    "INIT_LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Image into NUmPy array\n",
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None :\n",
    "            image = cv2.resize(image, default_image_size)   \n",
    "            return img_to_array(image)\n",
    "        else :\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
