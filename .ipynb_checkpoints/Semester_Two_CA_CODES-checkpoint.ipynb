{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42438f17",
   "metadata": {},
   "source": [
    "# Integrating Big Data Analytics and Convolutional Neural Networks for Pest and Disease Detection and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c174f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9764c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxrwxr-x+  - hduser supergroup          0 2024-03-13 11:10 /Cropdiseasedata\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-07 10:06 /output1\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-07 13:14 /output2\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-14 14:03 /user1\r\n"
     ]
    }
   ],
   "source": [
    "#List the contents of the root directory in HDFS\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea2ddca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sc master - running  locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bff725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55667e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 20:09:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "#Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"imageprocessing\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa030c",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82837bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 20:09:31 WARN FileSystem: Failed to initialize fileystem hdfs://Cropdiseasedata/CCMT-Dataset-Augmented/Cashew/train_set/anthracnose3102/*: java.lang.IllegalArgumentException: java.net.UnknownHostException: Cropdiseasedata\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "java.net.UnknownHostException: Cropdiseasedata",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3087/502871562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtraining_images_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"hdfs://Cropdiseasedata/CCMT-Dataset-Augmented/{crop}/train_set/{class_name}/*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mcrop_training_data_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_images_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mbinaryFiles\u001b[0;34m(self, path, minPartitions)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0mminPartitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminPartitions\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultMinPartitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         return RDD(\n\u001b[0;32m-> 1123\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinaryFiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminPartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m             \u001b[0mPairDeserializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUTF8Deserializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoOpSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: java.net.UnknownHostException: Cropdiseasedata"
     ]
    }
   ],
   "source": [
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose3102\", \"\tgumosis1714\", \"healthy5877\", \"leaf miner3466\", \"red rust4751\"],\n",
    "    \"Cassava\": [\"bacterial blight\", \"bacterial blight3241\", \"brown spot\", \"green mite\", \"healthy\",\"mosaic\"]\n",
    "}\n",
    "\n",
    "training_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_training_data_rdd = []\n",
    "    \n",
    "    for class_name in classes:\n",
    "        training_images_rdd = spark.sparkContext.binaryFiles(f\"hdfs://Cropdiseasedata/CCMT-Dataset-Augmented/{crop}/train_set/{class_name}/*\")\n",
    "        crop_training_data_rdd.append(training_images_rdd)\n",
    "        \n",
    "    training_data_rdds.append(crop_training_data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose3102\", \"\tgumosis1714\", \"healthy5877\", \"leaf miner3466\", \"red rust4751\"],\n",
    "    \"Cassava\": [\"bacterial blight\", \"bacterial blight3241\", \"brown spot\", \"green mite\", \"healthy\",\"mosaic\"],\n",
    "    \"Maize\": [\"class1\", \"class2\"],\n",
    "    \"Tomato\": [\"class1\", \"class2\", \"class3\", \"class4\", \"class5\"]\n",
    "}\n",
    "\n",
    "training_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_training_data_rdd = []\n",
    "    crop_testing_data_rdd = []\n",
    "\n",
    "    for class_name in classes:\n",
    "        training_images_rdd = spark.sparkContext.binaryFiles(f\"hdfs://Cropdiseasedata/CCMT-Dataset-Augmented/{crop}/train_set/{class_name}/*\")\n",
    "        crop_training_data_rdd.append(training_images_rdd)\n",
    "        \n",
    "    training_data_rdds.append(crop_training_data_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b442f2",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ae5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose\", \"gumosis\", \"healthy\", \"leaf miner\", \"red rust\"],\n",
    "    \"Cassava\": [\"class1\", \"class2\", \"class3\", \"class4\"],\n",
    "    \"Maize\": [\"class1\", \"class2\"],\n",
    "    \"Tomato\": [\"class1\", \"class2\", \"class3\", \"class4\", \"class5\"]\n",
    "}\n",
    "\n",
    "training_data_rdds = []\n",
    "testing_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_training_data_rdd = []\n",
    "    crop_testing_data_rdd = []\n",
    "\n",
    "    for class_name in classes:\n",
    "        training_images_rdd = spark.sparkContext.binaryFiles(f\"hdfs://Cropdiseasedata/CCMT-Dataset-Augmented/{crop}/train_set/{class_name}/*\")\n",
    "        testing_images_rdd = spark.sparkContext.binaryFiles(f\"hdfs://Cropdiseasedata/CCMT-Dataset-Augmented/{crop}/test_set/{class_name}/*\")\n",
    "        crop_training_data_rdd.append(training_images_rdd)\n",
    "        crop_testing_data_rdd.append(testing_images_rdd)\n",
    "\n",
    "    training_data_rdds.append(crop_training_data_rdd)\n",
    "    testing_data_rdds.append(crop_testing_data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SParkSession\n",
    "from pyspark.ml.image import ImageSchema\n",
    "#Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"imageprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Path to the directory containing the data in Hadoop\n",
    "crop_data = \"hdfs://9000/Cropdiseasedata/CCMT-Dataset-Augmented\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `spark` is a SparkSession\n",
    "crops = [\"crop1\", \"crop2\", \"crop3\", \"crop4\"]\n",
    "training_data_rdds = []\n",
    "testing_data_rdds = []\n",
    "\n",
    "for crop in crops:\n",
    "    training_images_rdd = spark.sparkContext.binaryFiles(f\"hdfs://path/to/{crop}/training/*\")\n",
    "    testing_images_rdd = spark.sparkContext.binaryFiles(f\"hdfs://path/to/{crop}/testing/*\")\n",
    "    training_data_rdds.append(training_images_rdd)\n",
    "    testing_data_rdds.append(testing_images_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b1cafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5db8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fc2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f44a636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Spark Context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"imageprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your image data\n",
    "cropdata = \"file:///home/hduser/Big data and advanced analytics data\"\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875f6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"localimageprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to your image data\n",
    "cropdata = \"file:///home/hduser/Big data and advanced analytics data/\"\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c816b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 20:17:57 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n",
      "+-----+\n",
      "|image|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is from the previous example.\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Path to your image data\n",
    "cropdata = \"file:///home/hduser/Big data and advanced analytics data\"\n",
    "\n",
    "# Read image files into a DataFrame\n",
    "crop_dataframe = spark.read.format(\"image\").load(cropdata)\n",
    "\n",
    "# Show the DataFrame schema and first few rows\n",
    "crop_dataframe.printSchema()\n",
    "crop_dataframe.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b42b0f07",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameReader' object has no attribute 'printSchema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5270/1846462966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The inferred schema can be visualized using the printSchema() method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcrop_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameReader' object has no attribute 'printSchema'"
     ]
    }
   ],
   "source": [
    "# The inferred schema can be visualized using the printSchema() method\n",
    "crop_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480df35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd6676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571a636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89eae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master ac4686d] comment\n",
      " 3 files changed, 240 insertions(+)\n",
      " create mode 100644 .ipynb_checkpoints/Semester_Two_CA_CODES-checkpoint.ipynb\n",
      " create mode 100644 .~lock.conference-template-a4.docx#\n",
      "Username for 'https://github.com': "
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"comment\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbfd102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#Import all libraries required for EDA, Preprocessing, Model building, Model Testing, Model EValuation and Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default size for input images\n",
    "width=256\n",
    "height=256\n",
    "depth=3epoch_ = 25\n",
    "BS = 32\n",
    "default_image_size = tuple((256, 256))\n",
    "image_size = 0\n",
    "root_dir = '/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/'\n",
    "INIT_LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Image into NUmPy array\n",
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None :\n",
    "            image = cv2.resize(image, default_image_size)   \n",
    "            return img_to_array(image)\n",
    "        else :\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image and Lable List\n",
    "image_list, label_list = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8dcf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635cbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf67f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec88f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillow Version: 9.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Pillow Version:\", PIL.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the images in a directory\n",
    "from os import listdir\n",
    "from matplotlib import image\n",
    "#Load all images i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79ea28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d17158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14cadff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb933559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e25851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
