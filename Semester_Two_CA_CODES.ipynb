{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42438f17",
   "metadata": {},
   "source": [
    "# Integrating Big Data Analytics and Convolutional Neural Networks for Pest and Disease Detection and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ead59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/hduser/.local/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch) (1.9)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: fsspec in /home/hduser/.local/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: networkx in /home/hduser/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: filelock in /home/hduser/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hduser/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hduser/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in /home/hduser/.local/lib/python3.10/site-packages (0.17.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: torch==2.2.1 in /home/hduser/.local/lib/python3.10/site-packages (from torchvision) (2.2.1)\n",
      "Requirement already satisfied: numpy in /home/hduser/.local/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (2.2.0)\n",
      "Requirement already satisfied: networkx in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: filelock in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (4.10.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch==2.2.1->torchvision) (1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch==2.2.1->torchvision) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/hduser/.local/lib/python3.10/site-packages (from torch==2.2.1->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/hduser/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision) (12.4.99)\n"
     ]
    }
   ],
   "source": [
    "#Install the necessary libraries\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d941a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9764c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 10:55 /crop_pest_disease_data\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-07 10:06 /output1\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-07 13:14 /output2\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-14 14:03 /user1\r\n"
     ]
    }
   ],
   "source": [
    "#List the contents of the root directory in HDFS\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ea2ddca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sc master - running  locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042a691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crop Pest Disease Detection\") \\\n",
    "    .getOrCreate()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad331f5",
   "metadata": {},
   "source": [
    "## processing the data using Apache Spark-PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a30ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "#Read image data from Hadoop using PySpark\n",
    "hadoop_data_path = \"hdfs://crop_pest_disease_data\"\n",
    "image_df = ImageSchema.readImages(hadoop_data_path)\n",
    "\n",
    "#Extract labels from folder names\n",
    "image_df = image_df.withColumn(\"label\", \n",
    "                                split(split(image_df[\"image\"][\"origin\"], \"/\")[size(split(image_df[\"image\"][\"origin\"], \"/\"))-2], \"_\")[size(split(split(image_df[\"image\"][\"origin\"], \"/\")[size(split(image_df[\"image\"][\"origin\"], \"/\"))-2], \"_\"))-1])\n",
    "\n",
    "#Perform Exploratory Data Analysis (EDA)\n",
    "class_distribution = image_df.groupBy(\"label\").count().orderBy(\"count\", ascending=False)\n",
    "class_distribution.show()\n",
    "\n",
    "#Display sample images\n",
    "image_df.show()\n",
    "\n",
    "#Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6567154",
   "metadata": {},
   "source": [
    "## processing the data using pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96919f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_data_path = \"hdfs://path/to/crop_pest_disease_data\" \n",
    "crop_df = datasets.ImageFolder(root=hadoop_data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e59238",
   "metadata": {},
   "source": [
    "## Perform Image processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2143f",
   "metadata": {},
   "source": [
    "The dataset has already been processed. Here is the image preprocessing that was done to the data.\n",
    "- All images were captured, separated,and saved in their respective folders according to the plant type.\n",
    "- The images were annotated and labelled\n",
    "- Image Cropping and size reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cba35",
   "metadata": {},
   "source": [
    "## Data Processing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6d5e7",
   "metadata": {},
   "source": [
    "### Checking the image sizes of each crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have already defined and loaded your dataset\n",
    "# dataset = datasets.ImageFolder(root=hadoop_data_path, transform=transform)\n",
    "\n",
    "# Get the list of classes (crops) in the dataset\n",
    "classes = dataset.classes\n",
    "\n",
    "# Create a dictionary to store image sizes for each crop\n",
    "crop_image_sizes = {crop: [] for crop in classes}\n",
    "\n",
    "# Iterate through the dataset\n",
    "for image_path, label in dataset.samples:\n",
    "    # Open the image using PIL\n",
    "    image = Image.open(image_path)\n",
    "    # Get the size of the image\n",
    "    width, height = image.size\n",
    "    # Get the crop name using the label\n",
    "    crop_name = classes[label]\n",
    "    # Append the image size to the corresponding crop in the dictionary\n",
    "    crop_image_sizes[crop_name].append((width, height))\n",
    "\n",
    "# Print the image sizes for each crop\n",
    "for crop, sizes in crop_image_sizes.items():\n",
    "    print(f\"Crop: {crop}\")\n",
    "    print(f\"Total images: {len(sizes)}\")\n",
    "    print(f\"Average image size: {sum([w for w, h in sizes]) / len(sizes)} x {sum([h for w, h in sizes]) / len(sizes)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeaea30",
   "metadata": {},
   "source": [
    "### check if the pixel value have been normalized to a range suitable for training neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641935e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Assuming you have already defined and loaded your dataset\n",
    "# dataset = datasets.ImageFolder(root=hadoop_data_path, transform=transform)\n",
    "\n",
    "# Define a transformation to convert PIL images to PyTorch tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load a few sample images from the dataset\n",
    "sample_loader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "for images, labels in sample_loader:\n",
    "    # Convert images to numpy arrays and print pixel value range\n",
    "    for image in images:\n",
    "        # Check pixel value range\n",
    "        min_pixel_value = torch.min(image)\n",
    "        max_pixel_value = torch.max(image)\n",
    "        print(f\"Min pixel value: {min_pixel_value}, Max pixel value: {max_pixel_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bd5d9",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define transformations for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load image data using PyTorch\n",
    "hadoop_data_path = \"hdfs://path/to/crop_pest_disease_data\"\n",
    "dataset = datasets.ImageFolder(root=hadoop_data_path, transform=transform)\n",
    "\n",
    "# Split data into training and testing datasets\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b16332",
   "metadata": {},
   "source": [
    "## Check if there are any defective images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9948d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=r'/kaggle/input/crop-pest-and-disease-detection'\n",
    "bad_img_list=[]\n",
    "total=0\n",
    "good=0\n",
    "bad=0\n",
    "classes=sorted(os.listdir(data_dir))\n",
    "for klass in classes:\n",
    "    good_class=0\n",
    "    bad_class=0\n",
    "    total_class=0\n",
    "    msg=f'processing class {klass}'\n",
    "    print(msg, '\\r', end= '')\n",
    "    classpath=os.path.join(data_dir, klass)\n",
    "    flist=sorted(os.listdir(classpath))\n",
    "    for f in flist:\n",
    "        total +=1\n",
    "        total_class +=1\n",
    "        fpath=os.path.join(classpath,f)\n",
    "        try:\n",
    "            img= Image.open(fpath) \n",
    "            array=np.asarray(img)\n",
    "            good +=1\n",
    "            good_class +=1\n",
    "        except:\n",
    "            bad_img_list.append(fpath)\n",
    "            bad +=1\n",
    "            bad_class +=1\n",
    "    \n",
    "    msg=f'class {klass} contains {total_class} files, {good_class} are valid image files and {bad_class} defective image files'\n",
    "    print (msg)\n",
    "msg=f'the dataset contains {total} image files, {good} are valid image files and {bad} are defective image files'\n",
    "print (msg)\n",
    "if bad>0:\n",
    "    ans=input('to print a list of defective image files enter P, to not print press Enter')\n",
    "    if ans == 'P' or ans == 'p':\n",
    "        for f in bad_img_list:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d62de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create a corrected dataset with the defective image files removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use this dataset to create a model.\n",
    "working_dir=r'/kaggle/working/'\n",
    "corrected_dir=os.path.join(working_dir, 'corrected dataset') # where the corrected dataset will be stored\n",
    "copied_count = 0\n",
    "if os.path.isdir(corrected_dir):\n",
    "    shutil.rmtree(corrected_dir) # make sure the corrected_dir is empty\n",
    "os.mkdir(corrected_dir)\n",
    "for klass in classes:\n",
    "    classpath=os.path.join(data_dir, klass)\n",
    "    dest_classpath=os.path.join(corrected_dir, klass)\n",
    "    os.mkdir(dest_classpath)\n",
    "    flist= os.listdir(classpath)\n",
    "    for f in flist:\n",
    "        fpath=os.path.join(classpath,f)\n",
    "        dest_fpath=os.path.join(dest_classpath,f)\n",
    "        if fpath not in bad_img_list:\n",
    "            shutil.copy(fpath, dest_fpath)\n",
    "            copied_count +=1\n",
    "msg=f'{copied_count} valid image files were stored in {corrected_dir}'\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the training data into training and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa030c",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose3102\", \"\tgumosis1714\", \"healthy5877\", \"leaf miner3466\", \"red rust4751\"],\n",
    "    \"Cassava\": [\"bacterial blight\", \"bacterial blight3241\", \"brown spot\", \"green mite\", \"healthy\",\"mosaic\"],\n",
    "    \"Maize\": [\"class1\", \"class2\"],\n",
    "    \"Tomato\": [\"class1\", \"class2\", \"class3\", \"class4\", \"class5\"]\n",
    "}\n",
    "\n",
    "train_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_train_data_rdd = []\n",
    "\n",
    "    for class_name in classes:\n",
    "        train_images_rdd = spark.sparkContext.binaryFiles(f\"hdfs://Cropdiseasedata/CCMT-Dataset-Augmented/{crop}/train_set/{class_name}/*\")\n",
    "        crop_train_data_rdd.append(train_images_rdd)\n",
    "        \n",
    "    train_data_rdds.append(crop_train_data_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc354a6",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b2c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32b442f2",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ae5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose\", \"gumosis\", \"healthy\", \"leaf miner\", \"red rust\"],\n",
    "    \"Cassava\": [\"class1\", \"class2\", \"class3\", \"class4\"],\n",
    "    \"Maize\": [\"class1\", \"class2\"],\n",
    "    \"Tomato\": [\"class1\", \"class2\", \"class3\", \"class4\", \"class5\"]\n",
    "}\n",
    "\n",
    "test_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_test_data_rdd = []\n",
    "\n",
    "    for class_name in classes:\n",
    "        test_images_rdd = spark.sparkContext.binaryFiles(f\"hdfs://Cropdiseasedata/CCMT-Dataset-Augmented/{crop}/train_set/{class_name}/*\")\n",
    "        crop_test_data_rdd.append(test_images_rdd)\n",
    "\n",
    "    testing_data_rdds.append(crop_testing_data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c816b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 20:17:57 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n",
      "+-----+\n",
      "|image|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is from the previous example.\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Path to your image data\n",
    "cropdata = \"file:///home/hduser/Big data and advanced analytics data\"\n",
    "\n",
    "# Read image files into a DataFrame\n",
    "crop_dataframe = spark.read.format(\"image\").load(cropdata)\n",
    "\n",
    "# Show the DataFrame schema and first few rows\n",
    "crop_dataframe.printSchema()\n",
    "crop_dataframe.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480df35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd6676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571a636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89eae45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master ac4686d] comment\n",
      " 3 files changed, 240 insertions(+)\n",
      " create mode 100644 .ipynb_checkpoints/Semester_Two_CA_CODES-checkpoint.ipynb\n",
      " create mode 100644 .~lock.conference-template-a4.docx#\n",
      "Username for 'https://github.com': "
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"comment\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbfd102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#Import all libraries required for EDA, Preprocessing, Model building, Model Testing, Model EValuation and Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default size for input images\n",
    "width=256\n",
    "height=256\n",
    "depth=3epoch_ = 25\n",
    "BS = 32\n",
    "default_image_size = tuple((256, 256))\n",
    "image_size = 0\n",
    "root_dir = '/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/'\n",
    "INIT_LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Image into NUmPy array\n",
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None :\n",
    "            image = cv2.resize(image, default_image_size)   \n",
    "            return img_to_array(image)\n",
    "        else :\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image and Lable List\n",
    "image_list, label_list = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8dcf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635cbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf67f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec88f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillow Version: 9.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Pillow Version:\", PIL.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the images in a directory\n",
    "from os import listdir\n",
    "from matplotlib import image\n",
    "#Load all images i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79ea28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d17158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14cadff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb933559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e25851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
