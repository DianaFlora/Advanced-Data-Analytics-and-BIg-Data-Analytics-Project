{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42438f17",
   "metadata": {},
   "source": [
    "# Integrating Big Data Analytics and Convolutional Neural Networks for Pest and Disease Detection and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ead59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install the necessary libraries\n",
    "#!pip install torch\n",
    "#!pip install torchvision\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d941a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9764c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 15:45 /crop_pest_disease_data\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-07 10:06 /output1\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-07 13:14 /output2\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-14 14:03 /user1\r\n"
     ]
    }
   ],
   "source": [
    "#List the contents of the root directory in HDFS\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78a57040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 12:05 /crop_pest_disease_data/Cashew\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 16:27 /crop_pest_disease_data/Cassava\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 18:00 /crop_pest_disease_data/Maize\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 13:04 /crop_pest_disease_data/Tomato\r\n"
     ]
    }
   ],
   "source": [
    "#List the contents of the crop pest and disease data directory in HDFS\n",
    "!hdfs dfs -ls /crop_pest_disease_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09944a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 12:02 /crop_pest_disease_data/Cashew/test_set\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 12:48 /crop_pest_disease_data/Cashew/train_set\r\n"
     ]
    }
   ],
   "source": [
    "#List the contents of Cashew in HDFS\n",
    "!hdfs dfs -ls /crop_pest_disease_data/Cashew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7055e52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 17:55 /crop_pest_disease_data/Cassava/test_set\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 17:12 /crop_pest_disease_data/Cassava/train_set\r\n"
     ]
    }
   ],
   "source": [
    "#List the contents of Cassava in HDFS\n",
    "!hdfs dfs -ls /crop_pest_disease_data/Cassava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34872d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 13:58 /crop_pest_disease_data/Maize/test_set\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 18:35 /crop_pest_disease_data/Maize/train_set\r\n"
     ]
    }
   ],
   "source": [
    "#List the contents of Maize in HDFS\n",
    "!hdfs dfs -ls /crop_pest_disease_data/Maize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a6945d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 12:59 /crop_pest_disease_data/Tomato/test_set\r\n",
      "drwxr-xr-x   - hduser supergroup          0 2024-03-20 13:30 /crop_pest_disease_data/Tomato/train_set\r\n"
     ]
    }
   ],
   "source": [
    "#List the contents of Tomato in HDFS\n",
    "!hdfs dfs -ls /crop_pest_disease_data/Tomato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad331f5",
   "metadata": {},
   "source": [
    "## Reading data from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8741ccae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose3102\", \"gumosis1714\", \"healthy5877\", \"leaf miner3466\", \"red rust4751\"],\n",
    "    \"Cassava\": [\"bacterial blight3241\", \"bacterial_blight\", \"brown_spot\", \"green_mite\", \"healthy\",\"mosaic\"],\n",
    "    \"Maize\": [\"fall armyworm\", \"grasshoper\", \"healthy\", \"leaf beetle\", \"leaf blight\", \"leaf_spot\",\"streak virus\"],\n",
    "    \"Tomato\": [\"healthy\", \"leaf blight\", \"leaf curl\", \"septoria leaf spot\",\"verticulium wilt\"]\n",
    "}\n",
    "\n",
    "train_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_train_data_rdd = []\n",
    "    for class_name in classes:\n",
    "        # Ensure proper concatenation of HDFS URI and class name\n",
    "\n",
    "        hdfs_path = f\"hdfs://localhost:9000/crop_pest_disease_data/{crop}/train_set/{class_name}\"\n",
    "        train_images_rdd = sc.binaryFiles(hdfs_path)\n",
    "        crop_train_data_rdd.append(train_images_rdd)\n",
    "    train_data_rdds.append(crop_train_data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cf9901e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data_rdds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c892bbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_ImageSchema' object has no attribute 'readImages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3923/2789083409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageSchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImageSchema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"hdfs://localhost:9000/crop_pest_disease_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: '_ImageSchema' object has no attribute 'readImages'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.image import ImageSchema\n",
    "ImageSchema.readImages(f\"hdfs://localhost:9000/crop_pest_disease_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa69568",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"image\").load(f\"hdfs://localhost:9000/crop_pest_disease_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a313d692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[org.apache.spark.api.java.JavaPairRDD@583a0574, org.apache.spark.api.java.JavaPairRDD@230e0f8d, org.apache.spark.api.java.JavaPairRDD@6e726d34, org.apache.spark.api.java.JavaPairRDD@2e244d48, org.apache.spark.api.java.JavaPairRDD@26bdec5d]\n",
      "[org.apache.spark.api.java.JavaPairRDD@759cfd60, org.apache.spark.api.java.JavaPairRDD@87a252a, org.apache.spark.api.java.JavaPairRDD@4b87760, org.apache.spark.api.java.JavaPairRDD@4daeacae, org.apache.spark.api.java.JavaPairRDD@5f4b3f86, org.apache.spark.api.java.JavaPairRDD@6411230]\n",
      "[org.apache.spark.api.java.JavaPairRDD@418566ef, org.apache.spark.api.java.JavaPairRDD@79f669ad, org.apache.spark.api.java.JavaPairRDD@7f47de1a, org.apache.spark.api.java.JavaPairRDD@71999350, org.apache.spark.api.java.JavaPairRDD@51b09846, org.apache.spark.api.java.JavaPairRDD@10462b64, org.apache.spark.api.java.JavaPairRDD@2df8392f]\n",
      "[org.apache.spark.api.java.JavaPairRDD@3f27c0a3, org.apache.spark.api.java.JavaPairRDD@6a801da9, org.apache.spark.api.java.JavaPairRDD@4bbc1696, org.apache.spark.api.java.JavaPairRDD@27e17f8f, org.apache.spark.api.java.JavaPairRDD@26c29108]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each element in the list train_data_rdds\n",
    "for element in train_data_rdds:\n",
    "  print_element(element)  # This will print or process each element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2846cfb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of object (6) does not match with length of fields (5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3923/1584158343.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create DataFrame with the adjusted schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_rdds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \"\"\"\n\u001b[0;32m--> 473\u001b[0;31m         return self.sparkSession.createDataFrame(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             )\n\u001b[0;32m-> 1276\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1277\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;31m# make sure data could consumed multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mno_type_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m                 \u001b[0mverify_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2001\u001b[0;31m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1970\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1972\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m   1973\u001b[0m                         new_msg(\n\u001b[1;32m   1974\u001b[0m                             \u001b[0;34m\"Length of object (%d) does not match with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of object (6) does not match with length of fields (5)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Assuming each element in train_data_rdds has 5 elements (adjust if different)\n",
    "schema = StructType([StructField(str(i), StringType(), True) for i in range(5)])\n",
    "\n",
    "# Create DataFrame with the adjusted schema\n",
    "df = sqlContext.createDataFrame(train_data_rdds, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store features and labels\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Iterate over each RDD\n",
    "for crop_rdd_list in train_data_rdds:\n",
    "    for rdd in crop_rdd_list:\n",
    "        # Extract crop name from RDD path\n",
    "        crop = rdd.name().split('/')[-3]\n",
    "        # Extract class name from RDD path\n",
    "        class_name = rdd.name().split('/')[-1]\n",
    "        # Extract features (images) and labels (class names) from file paths\n",
    "        features_with_paths = rdd.collect()\n",
    "        if features_with_paths:  # Check if RDD is not empty\n",
    "            features = [features for _, features in features_with_paths]\n",
    "            labels = [class_name] * len(features)\n",
    "            # Append features and labels to the lists\n",
    "            features_list.extend(features)\n",
    "            labels_list.extend(labels)\n",
    "\n",
    "# Now, features_list contains all the images and labels_list contains corresponding class names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc master - running  locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crop Pest Disease Detection\") \\\n",
    "    .getOrCreate()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aface619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_path) and os.listdir(class_path):  # Check if directory exists and not empty\n",
    "            for filename in os.listdir(class_path):\n",
    "                # Process image files here\n",
    "                # Example: load image using OpenCV or any other library\n",
    "                # image = cv2.imread(os.path.join(class_path, filename))\n",
    "                # images.append(image)\n",
    "                images.append(os.path.join(class_path, filename))  # Append image file path\n",
    "                labels.append(class_name)\n",
    "        else:\n",
    "            print(f\"Warning: Directory {class_path} is empty or does not exist.\")\n",
    "    return images, labels\n",
    "\n",
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose\", \"gumosis\", \"healthy\", \"leaf miner\", \"red rust\"],\n",
    "    \"Cassava\": [\"bacterial_blight\", \"brown_spot\", \"green_mite\", \"healthy\", \"mosaic\"],\n",
    "    \"Maize\": [\"fall armyworm\", \"grasshopper\", \"healthy\", \"leaf beetle\", \"leaf blight\", \"leaf_spot\", \"streak virus\"],\n",
    "    \"Tomato\": [\"healthy\", \"leaf blight\", \"leaf curl\", \"septoria leaf spot\", \"verticillium wilt\"]\n",
    "}\n",
    "\n",
    "data_directory = \"hdfs://localhost:9000/crop_pest_disease_data\"\n",
    "\n",
    "test_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_test_data_rdd = []\n",
    "    for class_name in classes:\n",
    "        hdfs_path = f\"{data_directory}/{crop}/test_set/{class_name}\"\n",
    "        images, labels = load_images_from_directory(hdfs_path)\n",
    "        if images and labels:  # Check if images and labels are not empty\n",
    "            crop_test_data_rdd.append((images, labels))\n",
    "    test_data_rdds.append(crop_test_data_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c490f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb626ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hadoop directory path containing image files\n",
    "hadoop_dir_path = \"hdfs://localhost:9000/crop_pest_disease_data\"\n",
    "\n",
    "# Read image files from Hadoop directory\n",
    "image_df = spark.read.format(\"image\").load(hadoop_dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc143565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a snippet of the DataFrame\n",
    "image_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d45743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View DataFrame schema\n",
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa70b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_df = datasets.ImageFolder(root=hadoop_data_path, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e59238",
   "metadata": {},
   "source": [
    "## Perform Image processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2143f",
   "metadata": {},
   "source": [
    "The dataset has already been processed. Here is the image preprocessing that was done to the data.\n",
    "- All images were captured, separated,and saved in their respective folders according to the plant type.\n",
    "- The images were annotated and labelled\n",
    "- Image Cropping and size reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cba35",
   "metadata": {},
   "source": [
    "## Data Processing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437d1c54",
   "metadata": {},
   "source": [
    "### Checking the image sizes of each crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have already defined and loaded your dataset\n",
    "# dataset = datasets.ImageFolder(root=hadoop_data_path, transform=transform)\n",
    "\n",
    "# Get the list of classes (crops) in the dataset\n",
    "classes = dataset.classes\n",
    "\n",
    "# Create a dictionary to store image sizes for each crop\n",
    "crop_image_sizes = {crop: [] for crop in classes}\n",
    "\n",
    "# Iterate through the dataset\n",
    "for image_path, label in dataset.samples:\n",
    "    # Open the image using PIL\n",
    "    image = Image.open(image_path)\n",
    "    # Get the size of the image\n",
    "    width, height = image.size\n",
    "    # Get the crop name using the label\n",
    "    crop_name = classes[label]\n",
    "    # Append the image size to the corresponding crop in the dictionary\n",
    "    crop_image_sizes[crop_name].append((width, height))\n",
    "\n",
    "# Print the image sizes for each crop\n",
    "for crop, sizes in crop_image_sizes.items():\n",
    "    print(f\"Crop: {crop}\")\n",
    "    print(f\"Total images: {len(sizes)}\")\n",
    "    print(f\"Average image size: {sum([w for w, h in sizes]) / len(sizes)} x {sum([h for w, h in sizes]) / len(sizes)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e23b5",
   "metadata": {},
   "source": [
    "### check if the pixel value have been normalized to a range suitable for training neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641935e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Assuming you have already defined and loaded your dataset\n",
    "# dataset = datasets.ImageFolder(root=hadoop_data_path, transform=transform)\n",
    "\n",
    "# Define a transformation to convert PIL images to PyTorch tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load a few sample images from the dataset\n",
    "sample_loader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "for images, labels in sample_loader:\n",
    "    # Convert images to numpy arrays and print pixel value range\n",
    "    for image in images:\n",
    "        # Check pixel value range\n",
    "        min_pixel_value = torch.min(image)\n",
    "        max_pixel_value = torch.max(image)\n",
    "        print(f\"Min pixel value: {min_pixel_value}, Max pixel value: {max_pixel_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bd5d9",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define transformations for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load image data using PyTorch\n",
    "hadoop_data_path = \"hdfs://path/to/crop_pest_disease_data\"\n",
    "dataset = datasets.ImageFolder(root=hadoop_data_path, transform=transform)\n",
    "\n",
    "# Split data into training and testing datasets\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b16332",
   "metadata": {},
   "source": [
    "## Check if there are any defective images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9948d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='hdfs://localhost:9000/crop_pest_disease_data'\n",
    "bad_img_list=[]\n",
    "total=0\n",
    "good=0\n",
    "bad=0\n",
    "classes=sorted(os.listdir(data_dir))\n",
    "for klass in classes:\n",
    "    good_class=0\n",
    "    bad_class=0\n",
    "    total_class=0\n",
    "    msg=f'processing class {klass}'\n",
    "    print(msg, '\\r', end= '')\n",
    "    classpath=os.path.join(data_dir, klass)\n",
    "    flist=sorted(os.listdir(classpath))\n",
    "    for f in flist:\n",
    "        total +=1\n",
    "        total_class +=1\n",
    "        fpath=os.path.join(classpath,f)\n",
    "        try:\n",
    "            img= Image.open(fpath) \n",
    "            array=np.asarray(img)\n",
    "            good +=1\n",
    "            good_class +=1\n",
    "        except:\n",
    "            bad_img_list.append(fpath)\n",
    "            bad +=1\n",
    "            bad_class +=1\n",
    "    \n",
    "    msg=f'class {klass} contains {total_class} files, {good_class} are valid image files and {bad_class} defective image files'\n",
    "    print (msg)\n",
    "msg=f'the dataset contains {total} image files, {good} are valid image files and {bad} are defective image files'\n",
    "print (msg)\n",
    "if bad>0:\n",
    "    ans=input('to print a list of defective image files enter P, to not print press Enter')\n",
    "    if ans == 'P' or ans == 'p':\n",
    "        for f in bad_img_list:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d62de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create a corrected dataset with the defective image files removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use this dataset to create a model.\n",
    "working_dir=r'/kaggle/working/'\n",
    "corrected_dir=os.path.join(working_dir, 'corrected dataset') # where the corrected dataset will be stored\n",
    "copied_count = 0\n",
    "if os.path.isdir(corrected_dir):\n",
    "    shutil.rmtree(corrected_dir) # make sure the corrected_dir is empty\n",
    "os.mkdir(corrected_dir)\n",
    "for klass in classes:\n",
    "    classpath=os.path.join(data_dir, klass)\n",
    "    dest_classpath=os.path.join(corrected_dir, klass)\n",
    "    os.mkdir(dest_classpath)\n",
    "    flist= os.listdir(classpath)\n",
    "    for f in flist:\n",
    "        fpath=os.path.join(classpath,f)\n",
    "        dest_fpath=os.path.join(dest_classpath,f)\n",
    "        if fpath not in bad_img_list:\n",
    "            shutil.copy(fpath, dest_fpath)\n",
    "            copied_count +=1\n",
    "msg=f'{copied_count} valid image files were stored in {corrected_dir}'\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the training data into training and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa030c",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc354a6",
   "metadata": {},
   "source": [
    "## Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b2c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32b442f2",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951ae5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_classes = {\n",
    "    \"Cashew\": [\"anthracnose\", \"gumosis\", \"healthy\", \"leaf miner\", \"red rust\"],\n",
    "    \"Cassava\": [\"bacterial_blight\", \"brown_spot\", \"green_mite\", \"healthy\",\"mosaic\"],\n",
    "    \"Maize\": [\"fall armyworm\", \"grasshoper\", \"healthy\", \"leaf beetle\", \"leaf blight\", \"leaf_spot\",\"streak virus\"],\n",
    "    \"Tomato\": [\"healthy\", \"leaf blight\", \"leaf curl\", \"septoria leaf spot\",\"verticulium wilt\"]\n",
    "}\n",
    "\n",
    "test_data_rdds = []\n",
    "\n",
    "for crop, classes in crops_classes.items():\n",
    "    crop_test_data_rdd = []\n",
    "    for class_name in classes:\n",
    "\n",
    "\n",
    "        hdfs_path = f\"hdfs://localhost:9000/crop_pest_disease_data/{crop}/test_set/{class_name}\"\n",
    "        test_images_rdd = spark.sparkContext.binaryFiles(hdfs_path)\n",
    "        crop_test_data_rdd.append(test_images_rdd)\n",
    "    test_data_rdds.append(crop_test_data_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db97b5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b786c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = f\"hdfs://localhost:9000/crop_pest_disease_data/Maize/test_set/grasshopper/0maize_valid_grasshoper.JPG\"\n",
    "test_images = spark.sparkContext.binaryFiles(hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eb4ec1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7576292b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://localhost:9000/crop_pest_disease_data/Maize/test_set/grasshopper/0maize_valid_grasshoper.JPG\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:340)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)\n\tat org.apache.spark.input.StreamFileInputFormat.setMinPartitions(PortableDataStream.scala:51)\n\tat org.apache.spark.rdd.BinaryFileRDD.getPartitions(BinaryFileRDD.scala:51)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Input path does not exist: hdfs://localhost:9000/crop_pest_disease_data/Maize/test_set/grasshopper/0maize_valid_grasshoper.JPG\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)\n\t... 26 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12263/3924687917.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://localhost:9000/crop_pest_disease_data/Maize/test_set/grasshopper/0maize_valid_grasshoper.JPG\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:340)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)\n\tat org.apache.spark.input.StreamFileInputFormat.setMinPartitions(PortableDataStream.scala:51)\n\tat org.apache.spark.rdd.BinaryFileRDD.getPartitions(BinaryFileRDD.scala:51)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.IOException: Input path does not exist: hdfs://localhost:9000/crop_pest_disease_data/Maize/test_set/grasshopper/0maize_valid_grasshoper.JPG\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:313)\n\t... 26 more\n"
     ]
    }
   ],
   "source": [
    "data=test_images.collect()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c816b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark is from the previous example.\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Path to your image data\n",
    "cropdata = \"file:///home/hduser/Big data and advanced analytics data\"\n",
    "\n",
    "# Read image files into a DataFrame\n",
    "crop_dataframe = spark.read.format(\"image\").load(cropdata)\n",
    "\n",
    "# Show the DataFrame schema and first few rows\n",
    "crop_dataframe.printSchema()\n",
    "crop_dataframe.show()\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480df35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd6676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571a636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89eae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"comment\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbfd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all libraries required for EDA, Preprocessing, Model building, Model Testing, Model EValuation and Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import cv2\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default size for input images\n",
    "width=256\n",
    "height=256\n",
    "depth=3epoch_ = 25\n",
    "BS = 32\n",
    "default_image_size = tuple((256, 256))\n",
    "image_size = 0\n",
    "root_dir = '/content/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/'\n",
    "INIT_LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Image into NUmPy array\n",
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv2.imread(image_dir)\n",
    "        if image is not None :\n",
    "            image = cv2.resize(image, default_image_size)   \n",
    "            return img_to_array(image)\n",
    "        else :\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image and Lable List\n",
    "image_list, label_list = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8dcf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635cbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec88f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pillow Version:\", PIL.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the images in a directory\n",
    "from os import listdir\n",
    "from matplotlib import image\n",
    "#Load all images i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79ea28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d17158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14cadff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb933559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e25851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
